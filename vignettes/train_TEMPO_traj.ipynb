{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2c14e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Optional deps\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# OT\n",
    "import ot  # pip install POT\n",
    "\n",
    "from trajectory_models import TrajectoryInference\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16be32a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = \"../data/dyngen.h5ad\"\n",
    "time_col = \"days_pd\"\n",
    "layer = None\n",
    "n_pcs = 64\n",
    "\n",
    "# --- training parameters ---\n",
    "batch_size = 1024\n",
    "steps = 50_000\n",
    "lr = 1e-3\n",
    "weight_decay = 0.0\n",
    "grad_clip = 1.0\n",
    "log_every = 200\n",
    "ckpt_every = 200\n",
    "eval_trajectories = 256\n",
    "\n",
    "\n",
    "sigma = \"adaptive4-1.0-0.0\"\n",
    "t_eps = 1e-3\n",
    "\n",
    "seed = 0\n",
    "device = \"auto\"               # \"auto\" or explicit \"cuda:n\"\n",
    "save_dir = \"./tempo_run\"\n",
    "\n",
    "resume = False                 # resume if ckpt exists\n",
    "resume_from = None            # explicit path, else save_dir/ckpt_last.pt\n",
    "\n",
    "# --- scheduler ---\n",
    "use_cosine = True\n",
    "min_lr = 1e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136840d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(args.seed)\n",
    "device = (torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "              if args.device == \"auto\" else torch.device(args.device))\n",
    "outdir = Path(args.save_dir).expanduser().resolve()\n",
    "outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Data ----------\n",
    "adata, X, t_raw = load_adata(args.adata, args.time_col, layer=args.layer)\n",
    "Xz, scaler, pca   = preprocess_features(X, n_pcs=args.n_pcs)\n",
    "t_norm, timepoints, _ = normalize_times(t_raw)\n",
    "X_by_t = split_by_time(Xz, t_norm, timepoints)\n",
    "print(f\"[data] n_timepoints={len(timepoints)}; dims={Xz.shape[1]}; counts={[len(x) for x in X_by_t]}\")\n",
    "\n",
    "plans   = compute_pairwise_ot_plans(X_by_t)\n",
    "sampler = ChainSampler(X_by_t, plans, rng=np.random.default_rng(args.seed))\n",
    "\n",
    "d       = Xz.shape[1]\n",
    "model, opt = build_model(d, args.lr, weight_decay=args.weight_decay)\n",
    "model.to(device)\n",
    "\n",
    "vel_model = TrajectoryInference(\n",
    "    sigma=(float(args.sigma) if args.sigma.replace('.', '', 1).isdigit() else args.sigma),\n",
    "    interpolation=args.interp\n",
    ")\n",
    "\n",
    "# ---------- Scheduler (optional) ----------\n",
    "sched = None\n",
    "if args.use_cosine:\n",
    "    sched = CosineAnnealingLR(opt, T_max=args.steps, eta_min=args.min_lr)\n",
    "\n",
    "# ---------- Resume ----------\n",
    "ckpt_path = Path(args.resume_from) if args.resume_from else (outdir / \"ckpt_last.pt\")\n",
    "start_step = 0\n",
    "loss_hist = []\n",
    "\n",
    "loss_file = outdir / \"training_loss.json\"\n",
    "if loss_file.exists():\n",
    "    try:\n",
    "        loss_hist = json.load(open(loss_file))\n",
    "    except Exception:\n",
    "        loss_hist = []\n",
    "\n",
    "if args.resume and ckpt_path.exists():\n",
    "    ckpt, start_step, loss_hist_from_ckpt = load_checkpoint(ckpt_path, model, opt, device)\n",
    "\n",
    "    # Restore sampler RNG if present\n",
    "    if \"sampler_bitgen_state\" in ckpt:\n",
    "        try:\n",
    "            sampler.rng.bit_generator.state = ckpt[\"sampler_bitgen_state\"]\n",
    "        except Exception:\n",
    "            sampler.rng = np.random.default_rng(args.seed)\n",
    "            sampler.rng.bit_generator.state = ckpt[\"sampler_bitgen_state\"]\n",
    "\n",
    "    # Try to infer step if the old ckpt didn't store it\n",
    "    if start_step == 0:\n",
    "        archived = sorted(outdir.glob(\"ckpt_step*.pt\"))\n",
    "        if archived:\n",
    "            try:\n",
    "                # files like ckpt_step0002000.pt -> 2000\n",
    "                start_step = max(int(p.stem.replace(\"ckpt_step\", \"\")) for p in archived)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # Continue LR schedule smoothly\n",
    "    if sched is not None and start_step > 0:\n",
    "        for _ in range(start_step):\n",
    "            sched.step()\n",
    "\n",
    "    print(f\"[resume] loaded {ckpt_path.name} at step {start_step}\")\n",
    "else:\n",
    "    print(\"[resume] starting fresh\")\n",
    "\n",
    "# ---------- Train ----------\n",
    "model.train()\n",
    "wall = time.time()\n",
    "loss_ema, ema_beta = None, 0.95\n",
    "\n",
    "for step in range(start_step + 1, args.steps + 1):\n",
    "    xs = torch.from_numpy(sampler.sample(args.batch_size)).to(device)  # [B, K+1, d]\n",
    "    B  = xs.shape[0]\n",
    "\n",
    "    # Anchors and safe t away from anchors\n",
    "    tp = np.asarray(timepoints, dtype=np.float32)\n",
    "    min_gap = float(np.min(np.diff(tp))) if len(tp) > 1 else 1.0\n",
    "    eps = min(args.t_eps, 0.25 * min_gap)\n",
    "\n",
    "    # Make writable tensor to avoid PyTorch warning\n",
    "    tp_tensor = torch.tensor(tp, dtype=torch.float32, device=device).expand(B, -1)\n",
    "    t_batch   = sample_times_away_from_anchors(B, tp, eps=eps, device=device)\n",
    "\n",
    "    t_out, xt, ut, _, _ = vel_model.sample_location_and_conditional_flow(xs, tp_tensor, t=t_batch)\n",
    "\n",
    "    # Safety checks\n",
    "    if (not torch.isfinite(xt).all()) or (not torch.isfinite(ut).all()):\n",
    "        print(f\"[warn@{step}] Non-finite xt/ut — skipping batch.\")\n",
    "        continue\n",
    "\n",
    "    pred = model(torch.cat([xt.squeeze(1), t_out.unsqueeze(1)], 1))\n",
    "    loss = ((pred - ut.squeeze(1)) ** 2).sum(1).mean()\n",
    "\n",
    "    if not torch.isfinite(loss):\n",
    "        print(f\"[warn@{step}] Non-finite loss — skipping batch.\")\n",
    "        continue\n",
    "\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    if args.grad_clip:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
    "    opt.step()\n",
    "    if sched is not None:\n",
    "        sched.step()\n",
    "\n",
    "    # ----- logging -----\n",
    "    loss_ema = loss.item() if loss_ema is None else ema_beta * loss_ema + (1 - ema_beta) * loss.item()\n",
    "    if step % args.log_every == 0:\n",
    "        lr_now = opt.param_groups[0][\"lr\"]\n",
    "        print(f\"[{step:>6d}/{args.steps}] loss={loss.item():8.3f}  ema={loss_ema:8.3f}  lr={lr_now:.2e}  ({time.time()-wall:.1f}s)\")\n",
    "        wall = time.time()\n",
    "    loss_hist.append(float(loss.item()))\n",
    "\n",
    "    # ----- checkpointing -----\n",
    "    if args.ckpt_every and (step % args.ckpt_every == 0):\n",
    "        meta = dict(\n",
    "            dimension=d,\n",
    "            timepoints=timepoints.tolist(),\n",
    "            sigma=args.sigma,\n",
    "            interpolation=args.interp,\n",
    "            # preprocessing\n",
    "            scaler_mean=scaler.mean_.tolist(),\n",
    "            scaler_scale=scaler.scale_.tolist(),\n",
    "            pca_components=(pca.components_.tolist() if pca is not None else None),\n",
    "            pca_mean=(pca.mean_.tolist() if pca is not None else None),\n",
    "            # model arch (for reloads)\n",
    "            x_latent_dim=128,\n",
    "            time_embed_dim=128,\n",
    "            conditional_model=False,\n",
    "            normalization=\"layernorm\",\n",
    "            activation=\"SELU\",\n",
    "            num_out_layers=3,\n",
    "            n_pcs=args.n_pcs,\n",
    "        )\n",
    "        tmp = outdir / \".ckpt_last.tmp\"\n",
    "        save_checkpoint(tmp, step, model, opt, loss_hist, meta, device, sampler=sampler)\n",
    "        os.replace(tmp, outdir / \"ckpt_last.pt\")  # atomic rename\n",
    "        # rolling archive without torch.load\n",
    "        shutil.copy2(outdir / \"ckpt_last.pt\", outdir / f\"ckpt_step{step:07d}.pt\")\n",
    "\n",
    "    if step % (args.log_every * 5) == 0:\n",
    "        json.dump(loss_hist, open(loss_file, \"w\"))\n",
    "\n",
    "# Final save\n",
    "final_meta = dict(\n",
    "    dimension=d,\n",
    "    timepoints=timepoints.tolist(),\n",
    "    sigma=args.sigma,\n",
    "    interpolation=args.interp,\n",
    "    scaler_mean=scaler.mean_.tolist(),\n",
    "    scaler_scale=scaler.scale_.tolist(),\n",
    "    pca_components=(pca.components_.tolist() if pca is not None else None),\n",
    "    pca_mean=(pca.mean_.tolist() if pca is not None else None),\n",
    "    x_latent_dim=128,\n",
    "    time_embed_dim=128,\n",
    "    conditional_model=False,\n",
    "    normalization=\"layernorm\",\n",
    "    activation=\"SELU\",\n",
    "    num_out_layers=3,\n",
    "    n_pcs=args.n_pcs,\n",
    ")\n",
    "# final checkpoint with optimizer (useful for later resume)\n",
    "save_checkpoint(outdir / \"ckpt_last.pt\", step, model, opt, loss_hist, final_meta, device, sampler=sampler)\n",
    "json.dump(loss_hist, open(loss_file, \"w\"))\n",
    "\n",
    "# Evaluate a few trajectories (shape: X: [B, d])\n",
    "X0 = X_by_t[0][: min(args.eval_trajectories, len(X_by_t[0]))].astype(np.float32)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    traj = sample_trajectory(\n",
    "        model,\n",
    "        X=torch.from_numpy(X0),\n",
    "        y=torch.zeros(len(X0)),\n",
    "        device=device,\n",
    "        guidance=1.0,\n",
    "        conditional_model=False,\n",
    "        steps=1001,\n",
    "        method=\"dopri5\",\n",
    "    )\n",
    "np.save(outdir / \"trajectories.npy\", traj)\n",
    "print(f\"Saved to: {outdir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mmfm)",
   "language": "python",
   "name": "mmfm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
